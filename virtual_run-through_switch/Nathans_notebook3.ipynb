{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Epoch [1/10], Loss: 1.0585, Accuracy: 39.13%\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Epoch [2/10], Loss: 0.9395, Accuracy: 55.90%\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Epoch [3/10], Loss: 0.9017, Accuracy: 52.48%\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Epoch [4/10], Loss: 0.9105, Accuracy: 53.73%\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Epoch [5/10], Loss: 0.8209, Accuracy: 56.83%\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Epoch [6/10], Loss: 0.7488, Accuracy: 64.29%\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Epoch [7/10], Loss: 0.7008, Accuracy: 69.25%\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Epoch [8/10], Loss: 0.6179, Accuracy: 74.53%\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Epoch [9/10], Loss: 0.4688, Accuracy: 80.43%\n",
      "Warning: No annotations for image rs00207.jpg, skipping.\n",
      "Warning: No annotations for image rs00204.jpg, skipping.\n",
      "Warning: No annotations for image rs00205.jpg, skipping.\n",
      "Warning: No annotations for image rs00210.jpg, skipping.\n",
      "Warning: No annotations for image rs00208.jpg, skipping.\n",
      "Warning: No annotations for image rs00211.jpg, skipping.\n",
      "Warning: No annotations for image rs00209.jpg, skipping.\n",
      "Epoch [10/10], Loss: 0.3998, Accuracy: 85.09%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Paths\n",
    "annotation_file = r\"C:/Users/natha/OneDrive/Desktop/Applied Machine Learning/Run-Through_Switch/annotations/instances_default.json\"\n",
    "image_dir = r\"C:/Users/natha/OneDrive/Desktop/Applied Machine Learning/Run-Through_Switch/images\"\n",
    "output_dir = r\"C:/Users/natha/OneDrive/Desktop/Applied Machine Learning/Run-Through_Switch/processed_data\"\n",
    "\n",
    "# Load annotations\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Define a custom dataset for loading images and their annotations\n",
    "class RailroadTrackDataset(Dataset):\n",
    "    def __init__(self, coco, image_dir, transform=None):\n",
    "        self.coco = coco\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.img_ids = coco.getImgIds()  # Get all image IDs\n",
    "        \n",
    "        # Map category IDs to labels\n",
    "        self.label_map = {\n",
    "            \"open_switch\": 0,\n",
    "            \"closed_swtich\": 1,\n",
    "            \"straight\": 2\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get annotations (category IDs and bounding boxes)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Check if annotations exist for this image\n",
    "        if len(anns) == 0:\n",
    "            print(f\"Warning: No annotations for image {img_info['file_name']}, skipping.\")\n",
    "            return None  # Skip this sample (or you could return a default image/label if needed)\n",
    "        \n",
    "        # Get the category label (assuming a single label per image)\n",
    "        category_id = anns[0]['category_id']\n",
    "        category_name = self.coco.loadCats(category_id)[0]['name']\n",
    "        label = self.label_map[category_name]\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))  # Filter out None samples\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "dataset = RailroadTrackDataset(coco, image_dir, transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define a CNN model for classification\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 classes: open_switch, closed_swtich, straight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set up device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        if images is None or labels is None:\n",
    "            continue  # Skip this batch if it's None\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"railroad_track_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: straight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_24768\\3272471161.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('railroad_track_cnn.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Reinitialize the model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 classes: open_switch, closed_swtich, straight\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Load the trained model weights\n",
    "model = CNNModel()\n",
    "model.load_state_dict(torch.load('railroad_track_cnn.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Step 3: Define the same transformations used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Step 4: Load a test image\n",
    "test_image_path = r\"C:/Users/natha/Downloads/test2.jpg\"  # Replace with your test image path\n",
    "image = Image.open(test_image_path).convert('RGB')\n",
    "\n",
    "# Apply the same transformations to the image\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Step 5: Make predictions\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    output = model(image)  # Get model output\n",
    "    _, predicted = torch.max(output, 1)  # Get the class with the highest score\n",
    "\n",
    "# Step 6: Map the prediction index to the corresponding class name\n",
    "label_map = {0: 'open_switch', 1: 'closed_swtich', 2: 'straight'}\n",
    "predicted_class = label_map[predicted.item()]\n",
    "\n",
    "# Step 7: Output the prediction\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_run-through_switch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
